\chapter{Methodology}
\label{ch:intro}

\section{System Overview}

The architecture overview and the complete step-by-step algorithm flow of the proposed localization system are shown in Figure \ref{fig:diagram-map-basedlocalization} and Algorithm \ref{alg:localization-system}. The input of the system includes the raw online point-cloud, raw IMU data as well as the prior map , and output real-time accurate 6 DOF pose.\\	
The system can be divided in to the following different modules:
\begin{enumerate}
	\item Scan Pre-Processing: Performs a series of point cloud processing steps to filter and extract relevant features from the raw LiDAR data.
	
	\item LIDAR-INERTIA Odometry - Estimates high-frequency odometry by fusing raw IMU data and LiDAR scans using FAST-LIO\cite{xuFastLIO2} framework. The resulting odometry is added as a local constraint (odom factor) to the factor graph.
	
	\item Dynamic Local Map Loading: Generates a local map around the robot’s current pose within a specified radius from the prior map.
	
	
	\item Scan-to-Map Matching: Aligns the current LiDAR scan with the local map to estimate the robot's pose in the prior map using parallelize NDT. The estimated pose is added as a prior constraint (map factor) to the factor graph.
	
	\item Sliding Window Factor Graph Optimizer:Fuses odometry and map constraints within a sliding window to optimize and output a real-time, accurate 6-DoF pose.
	
	
\end{enumerate}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/system-overview.png}
    \caption{Complete Diagram of Map Based Localization}
    \label{fig:diagram-map-basedlocalization}
\end{figure}


\begin{algorithm}[H]
	\caption{LiDAR-Inertial Localization with Prior Map}
	\label{alg:localization-system}
	
	\textbf{Input:} $\mathcal{P}_t$ (LiDAR Point Cloud), $\mathcal{I}_t$ (IMU Data), $\mathcal{M}$ (Prior Map)\\
	\textbf{Output:} $\mathbf{T}_t$ (Real-time 6-DoF Pose)
	
	\begin{algorithmic}[1]
		
		\While{System is running}
		
		\State Acquire LiDAR point cloud $\mathcal{P}_t$ and IMU data $\mathcal{I}_t$
		
		\State Perform scan pre-processing on $\mathcal{P}_t$ to obtain filtered point cloud $\mathcal{P}_t^{filtered}$
		
		\State Estimate LiDAR-Inertial Odometry using $\{\mathcal{P}_t^{filtered}, \mathcal{I}_t\}$ to obtain odometry pose $\mathbf{T}_t^{lio}$	
		
		\If{$|| \mathbf{T}_t - \mathbf{T}_{last}^{map} || > d_{threshold}$}
		\State Load dynamic local map $\mathcal{M}_t^{local}$ from $\mathcal{M}$ centered at $\mathbf{T}_t^{lio}$
		\State Update $\mathbf{T}_{last}^{map} \gets \mathbf{T}_t$
		\EndIf
		
		\State Perform scan-to-map matching between $\mathcal{P}_t^{filtered}$ and $\mathcal{M}_t^{local}$ to obtain map-based pose $\mathbf{T}_t^{map}$
		
		\If{Matching score $> s_{threshold}$}
		\State Add prior factor with $\mathbf{T}_t^{map}$ to factor graph
		\EndIf
		
		\State Add odometry factor with $\mathbf{T}_t^{lio}$ to factor graph
		
		\State Optimize sliding window factor graph to obtain final pose $\mathbf{T}_t$
		
		\EndWhile
		
	\end{algorithmic}
\end{algorithm}


\section{LIDAR Scan Pre-Processing}
Before aligning each incoming LiDAR scan to the global map, we first subject the raw point cloud to a series of pre-processing operations. These operations improve both efficiency and reliability by filtering out irrelevant regions and outliers, down-sampling to reduce data volume, and transforming the data into a consistent coordinate frame.Our approach, illustrated in Figure\ref{fig:lidar_scan_preprocessing}  consists of the following  main steps:
\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{images/LIDAR_PreProccess.drawio.png}
    \caption{LIDAR scan pre-processing}
    \label{fig:lidar_scan_preprocessing}
\end{figure}

\begin{itemize}
    \item \textbf{Crop-Box Filter}: This step confines the point cloud to a user-defined three-dimensional region (a bounding “box”) around the sensor or area of interest. By discarding all points lying outside this box, we focus only on the relevant portion of the scene, reducing both memory and computational overhead for subsequent processing.
    \item \textbf{Outlier Removal}: Once the point cloud is restricted to the area of interest, we remove random noise and spurious measurements using a statistical outlier filter. Concretely, we compute for each point the average distance to its k‐nearest neighbors. Points whose mean distance exceeds the global mean plus a chosen multiplier of the standard deviation are marked as outliers and deleted. This practice is a commonly adopted heuristic that discards stray or floating points, which might result from occasional sensor glitches or moving objects in the environment.
    \item \textbf{Voxel-Grid Downsampling} To further ease computational load, the point cloud is then down-sampled. A typical approach uses a voxel-grid filter: space is subdivided into small volumetric “voxels,” and all points within each voxel are replaced by a single centroid representative. This retains the main geometry while reducing point density, thereby speeding up map‐based matching algorithms.
    \item \textbf{Coordinate Transformation}:   Finally, we apply a rigid transformation that aligns the processed point cloud to a consistent coordinate frame used throughout our localization pipeline—the vehicle’s base link frame.This accounts for any known extrinsic offsets between the LiDAR sensor and the vehicle origin.
\end{itemize}

As summarized in Table~\ref{tab:lidar_preprocessing}, the LiDAR preprocessing parameters vary depending on the sensor type and application requirements. All filtering and scan processing operations are implemented using the Point Cloud Library (PCL).



\begin{table}[htbp]
\centering
\caption{LiDAR Scan Processing Methods and Parametrs  Across Different Sensor Setups}
\label{tab:lidar_preprocessing}
\begin{tabular}{|p{3.5cm}|p{3.5cm}|p{3.5cm}|p{3.5cm}|}
\hline
\textbf{Processing Step} & \textbf{KITTI(Velodyne HDL-64E)} & \textbf{MulRan(Ouster-OS1-64)} & \textbf{(Ouster-OS1-128)} \\
\hline
LiDAR Type & Velodyne HDL-64E & Ouster OS1-64 & Ouster OS1-128 \\
% \hline
% \textbf{Vertical Channels} & 64 & 64 & 128 \\
% \hline
% \textbf{Vertical FOV} & $\pm$2° to $\pm$24.8° ($\sim$26.8° total) & $\pm$16.6° ($\sim$33.2° total) & $\pm$22.5° ($\sim$45° total) \\
\hline
Max Range & $\sim$120 m & $\sim$120 m & $\sim$240 m \\
\hline
Crop Box Filter & 90-100m & 90-100m & 180-200m\\
\hline
Downsampling Method & Voxel-Grid(leaf-size: 0.5m) & Voxel-Grid(leaf-size: 0.4-0.6 m) & Voxel-Voxel(0.5-0.8 m) \\
\hline
Outlier Removal & Statistical (K=50, Std=1.0) & Statistical (K=40, Std=1.0) & Statistical(K=40 ,Std=1,0) \\
\hline
% \textbf{Ground Removal} & Not used & Yes — flat surface segmentation &  Yes — flat surface segmentation \\
% \hline
\end{tabular}
\end{table}

\subsection{Map Pre-Processing}

To support real-time localization in large-scale environments, the global point-cloud map is pre-processed into a 2D grid of “tiles,” each stored as an individual point-cloud file. During this offline division, every point’s \(x\)–\(y\) coordinates are used to assign it to a grid cell of side length \(s\), where \(s\) is chosen to balance tile granularity against file count.Each
grid cell is saved as an individual point cloud file, and its index is stored as metadata for fast
retrieval. During localization, these indexes enable loading only the relevant grids within the
robot’s local map radius, reducing memory usage and improving processing efficiency.



At runtime, we maintain a dynamic local sub-map around the robot by loading only those tiles whose centers lie within a radius
\begin{equation}
	\label{eq:rmap}
	r_{\mathrm{map}} = r_{\mathrm{lidar}} + r_{\mathrm{margin}},
\end{equation}
where \(r_{\mathrm{lidar}}\) is the LiDAR’s maximum range and \(r_{\mathrm{margin}}\) is a small buffer to ensure sufficient overlap for robust scan-to-map matching. To avoid unnecessary I/O, we trigger a map update only when the robot’s current position \(\mathbf{T}_t\) moves farther than a threshold \(d_{\mathrm{thresh}}\) from the last update location \(\mathbf{T}_{\mathrm{last}}\), that is,
\begin{equation}
	\label{eq:load_condition}
	\|\mathbf{T}_t - \mathbf{T}_{\mathrm{last}}\|_2 > d_{\mathrm{thresh}}.
\end{equation}

When equation~\ref{eq:load_condition} is satisfied, the system identifies all grid cells whose centers fall within the circle of radius \(r_{\mathrm{map}}\) about \(\mathbf{T}_t\). Those tiles not yet in memory are loaded from the global map, while any previously loaded tiles that now lie outside this radius are evicted. Finally, \(\mathbf{T}_{\mathrm{last}}\) is updated to the new robot position. This selective, radius-based loading and unloading scheme keeps the local map size bounded, minimizes memory usage, and ensures that scan-to-map matching always operates on the most relevant portion of the prior map.

\section{LIDAR Inertia Odometry} 
FAST-LIO2 serves as the high-frequency LiDAR–Inertial Odometry front-end in our localization pipeline. At each timestep \(t\), the filter ingests IMU angular rate \(\omega_m(t)\) and linear acceleration \(a_m(t)\) (200\,Hz) together with motion-compensated LiDAR point clouds (10\,Hz), deskewed using the same IMU measurements. All sensor data are transformed into a common body frame via the known extrinsic calibration 
\begin{equation}
	T_{\mathrm{imu}\to\mathrm{lidar}}
	\;\in\;SE(3).
\end{equation}
FAST-LIO2 implements a tightly-coupled iterated Kalman filter, directly associating each incoming LiDAR point with the local map using an incremental k-d tree (ikd-Tree), thereby eliminating explicit feature extraction and frame-to-frame scan matching. At each LiDAR update, the filter propagates the state estimate \(\mathbf{x}(t)\) via the IMU pre-integration equations, then corrects it using the residuals between measured points and their expected positions in the map. The result is a 6-DOF pose estimate
\begin{equation}
	\hat{\mathbf{T}}_{WB}(t)
	\;=\;
	\begin{bmatrix}
		\mathbf{R}_{WB}(t) & \mathbf{t}_{WB}(t) \\
		\mathbf{0}           & 1
	\end{bmatrix}
	\;\in\;SE(3),
\end{equation}
which we publish as an odometry factor in our global factor graph. IMU noise densities and bias random-walk coefficients (\(\sigma_\omega, \sigma_a, \sigma_{b_\omega}, \sigma_{b_a}\)) are set according to the manufacturer’s specifications. While FAST-LIO2 exhibits negligible drift over short intervals, residual error on long trajectories is corrected through map matching constraints in the subsequent optimization (Section~\ref{sec:fusion}).


\section{Scan-to-Map Matching}

The Scan-to-Map Matching module is a crucial component of the proposed localization system. Its primary objective is to align the current LiDAR scan with the dynamically loaded local map and estimate the robot's accurate pose within the prior map coordinate frame.In this work, the Normal Distributions Transform (NDT)\cite{biber2003ndt} algorithm is employed for robust and efficient scan registration. Specifically, a multi-threaded implementation of NDT (NDT-OMP) \cite{koide2019portable} is utilized, leveraging OpenMP-based parallelization to accelerate computation and enable real-time performance. 



The module begins by voxelizing the local map: each occupied cell \(i\) is represented by a Gaussian distribution \(\mathcal{N}(\mu_i,\Sigma_i)\) at a resolution \(r\) that balances map fidelity against computational load. Given an initial pose estimate \(\mathbf{p}_0 = [x_0,y_0,z_0,\phi_0,\theta_0,\psi_0]^T\) (from the previous fused pose), the algorithm refines this estimate by minimizing the negative log-likelihood of the transformed scan points under their enclosing Gaussians (more detail Section~\ref{sec:Normal Distributions Transform}):
\[
J(\mathbf{p}) = -\sum_{k=1}^N \log\bigl[\mathcal{N}\bigl(T(\mathbf{p},\mathbf{x}_k)\mid\mu_{c(k)},\Sigma_{c(k)}\bigr)\bigr].
\]


To meet real-time constrain, the computation of gradient(first derivative) and Hessian(second derivative) of the cost function are parallelized over T threads at each Newton iteration (up to \(N_{\max}\)).The set of LIDAR scan points \(\{\mathbf{x}_k\}_{k=1}^N\) partitioned into disjoint subsets \(\{\mathcal{K}_t\}_{t=1}^T\). Each thread \(t\) independently accumulates local gradient and Hessian contributions:

\begin{align}
	\label{eq:local-gradient}
	\mathbf{g}^{(t)}
	&= \sum_{k\in\mathcal{K}_t} \nabla_{\!p}\bigl[-\log p(T(\mathbf{p},\mathbf{x}_k))\bigr],\\
	\label{eq:local-hessian}
	\mathbf{H}^{(t)}
	&= \sum_{k\in\mathcal{K}_t} \nabla^2_{\!p}\bigl[-\log p(T(\mathbf{p},\mathbf{x}_k))\bigr].
\end{align}

After all threads complete, a final reduction merges these into the global quantities.This lock-free design uses per-thread buffers and OpenMP’s guided scheduling to balance uneven voxel densities.


\begin{equation}
	\label{eq:reduction}
	\mathbf{g} = \sum_{t=1}^T \mathbf{g}^{(t)},
	\quad
	\mathbf{H} = \sum_{t=1}^T \mathbf{H}^{(t)}.
\end{equation}


At each iteration the pose update uses a line-search–scaled Newton step:

\begin{equation}
	\label{eq:update}
	\mathbf{p} \;\leftarrow\; \mathbf{p} \;-\; \alpha \,\mathbf{H}(\mathbf{p})^{-1}\,\mathbf{g}(\mathbf{p}),
	\quad
	\alpha \;\in\; [0,1], 
	\quad
	\|\Delta\mathbf{p}\|<\varepsilon.
\end{equation}
Table ~\ref{tab:ndt-omp-params} Describes the main parameters and their value in our implementation.
\begin{table}[ht]
	
	\centering
	\renewcommand{\arraystretch}{1}
	\setlength{\tabcolsep}{2pt}
	\caption{Key parameters of the NDT-OMP Scan-to-Map Matching implementation}
	\label{tab:ndt-omp-params}
	\begin{tabular}{@{}llll@{}}
		\toprule
		\textbf{Parameter}         & \textbf{Symbol}       & \textbf{Description}                                      & \textbf{Value}                          \\ 
		\midrule
		Voxel resolution           & \(r\)                 & Side length of each map voxel                             & 2\,m                                    \\ 
		Initial step size          & \(\alpha_0\)          & Starting multiplier for Newton update                     & 0.2                                    \\ 
		Convergence threshold      & \(\varepsilon\)       & Minimum norm of pose update for convergence                & \(10^{-4}\)                             \\ 
		Maximum iterations         & \(N_{\max}\)          & Upper limit on Newton-method iterations                    & 40                                   \\ 
		Thread count               & \(T\)                 & Number of parallel threads (OpenMP maximum)               & 8       \\ 
		\bottomrule
	\end{tabular}
\end{table}





\section{ Real-Time Pose Graph Optimization }
\label{sec:fusion}

The fusion strategy combines high-frequency odometry from LIO with global pose corrections from scan-to-map matching. An incremental solver (iSAM2) with fixed-lag smoothing is used to maintain real-time performance while optimizing recent trajectory estimates. This approach ensures accurate and drift-resilient pose estimation while keeping computational cost bounded.

\subsection{Factor Graph Formulation}

We denote the robot trajectory as a sequence of poses:
\begin{equation}
	\mathcal{X} = { \mathbf{x}_0, \mathbf{x}_1, \dots, \mathbf{x}_k }, \quad \mathbf{x}_i \in SE(3)
\end{equation}
where each pose $\mathbf{x}i $  represents the 6-DOF position and orientation of the robot at discrete time step \textit{i} , and SE(3) denotes the special Euclidean group representing 3D rigid body transformations.

\paragraph{1. Prior Factor}
A prior factor is used to anchor the initial pose to a known global reference frame. It models the prior distribution over the initial state , expressed as:
\begin{equation}
	\phi_{\text{prior}}(\mathbf{x}_0) = \exp\left( -\frac{1}{2} | \mathbf{x}_0 - \bar{\mathbf{x}}_0 |^2{\Sigma_0} \right)
\end{equation}

In this formulation, $\mathbf{x}_0$ is the estimated initial pose of the robot. The term $\bar{\mathbf{x}}_0$ denotes the prior mean pose from the first LIO output or initialization, and $\mathbf{{\Sigma_0}}$ is the prior covariance matrix reflecting the uncertainty in. This Gaussian prior serves as a fixed anchor to stabilize the trajectory estimation.

\paragraph{2. Odometry (Between) Factors}
Relative motion from LIO is encoded using between factors, which represent the relative transformation between consecutive poses:
\begin{equation}
	\phi_{\text{odom}}(\mathbf{x}{i-1}, \mathbf{x}i) = \exp\left( -\frac{1}{2} \left| f{\text{odom}}(\mathbf{x}_{i-1}, \mathbf{x}i) - \bar{\mathbf{z}}i^{\text{odom}} \right|^2{\Sigma{\text{odom}}} \right)
\end{equation}
Here,$\bar{\mathbf{z}}i^{\text{odom}}$ is the relative pose measurement between time steps $\text{\textit{i}}$  and $\text{\textit{i+1}}$, derived from the LIO module. The $f{\text{odom}(\mathbf{.})}$
function  calculates the predicted relative transformation between the estimated poses $\mathbf{x}_{i-1}$ and $\mathbf{x}_{i},$ . The $ \mathbf{\Sigma{\text{odom}}}$ term   is the associated covariance matrix, which quantifies the uncertainty in the odometry measurement. These between factors form the sequential backbone of the trajectory by chaining consecutive pose estimates together through relative motion constraints.

\paragraph{3. Map-Matching Priors}
Absolute pose estimates derived from scan-to-map matching using Normal Distributions Transform (NDT) are incorporated as prior constraints on select poses:
\begin{equation}
	\phi_{\text{map}}(\mathbf{x}_j) = \exp\left( -\frac{1}{2} | \mathbf{x}j - \bar{\mathbf{x}}j^{\text{map}} |^2{\Sigma{\text{map}}} \right)
\end{equation}
In this expression, $\bar{\mathbf{x}}j^{\text{map}} $  denotes the global pose obtained by aligning the current LiDAR scan with a pre-built prior map using scan matching techniques. The associated covariance matrix $\mathbf{\Sigma{\text{map}}}$ expresses the uncertainty of the registration process, accounting for alignment quality and sensor noise. These priors provide absolute spatial constraints within the factor graph, which help to correct accumulated drift from relative odometry and enhance the global consistency of the trajectory estimation.
\subsection{Maximum A Posteriori (MAP) Estimation}

The objective of the sensor fusion system is to compute the most probable robot trajectory given all available measurements, including odometry and map-matching observations. This is formulated as a Maximum A Posteriori (MAP) estimation problem, expressed as the following nonlinear least-squares objective:

\begin{equation}
	\mathbf{X}^* = \arg\min_{\mathbf{X}} \left(
	\| \mathbf{x}_0 - \bar{\mathbf{x}}_0 \|^2_{\Sigma_0} +
	\sum_{i=1}^{k} \left\| f_{\text{odom}}(\mathbf{x}_{i-1}, \mathbf{x}_i) - \bar{\mathbf{z}}_i^{\text{odom}} \right\|^2_{\Sigma_{\text{odom}}} +
	\sum_{j \in \mathcal{M}} \left\| \mathbf{x}_j - \bar{\mathbf{x}}_j^{\text{map}} \right\|^2_{\Sigma_{\text{map}}}
	\right)
	\label{eq:map-estimation}
\end{equation}

\begin{comment}
	Here:
	\begin{itemize}
		\item \( \mathbf{X} = \{ \mathbf{x}_0, \mathbf{x}_1, \dots, \mathbf{x}_k \} \) is the sequence of robot poses to be estimated.
		\item \( \bar{\mathbf{x}}_0 \) is the prior estimate of the initial pose, with covariance \( \Sigma_0 \).
		\item \( \bar{\mathbf{z}}_i^{\text{odom}} \) is the observed relative transformation between \( \mathbf{x}_{i-1} \) and \( \mathbf{x}_i \) from LiDAR-inertial odometry, with associated covariance \( \Sigma_{\text{odom}} \).
		\item \( f_{\text{odom}}(\cdot) \) is the motion model predicting the relative transformation between consecutive poses.
		\item \( \bar{\mathbf{x}}_j^{\text{map}} \) represents the pose obtained from map-matching at selected indices \( j \in \mathcal{M} \), with uncertainty \( \Sigma_{\text{map}} \).
	\end{itemize}
\end{comment}

Each term in the cost function represents a Mahalanobis distance, penalizing deviations from the respective measurements weighted by their uncertainty. Solving this objective yields the optimal trajectory estimate \( \mathbf{X}^* \), which fuses relative odometry and global pose observations in a probabilistically consistent manner.


\subsection{Fixed-Lag Smoothing and Marginalization}

In our proposed system, fixed-lag smoothing is used to maintain real-time performance while optimizing the recent trajectory of the robot. Instead of keeping the entire history of poses in the factor graph, we define a sliding time window of length \( \tau \), such that only poses within the interval \( [t_k - \tau, t_k] \) are retained, where \( t_k \) is the current time. This ensures that the graph grows only within a fixed temporal horizon, regardless of how long the system has been running.

Let \( \mathbf{x}_0, \mathbf{x}_1, \dots, \mathbf{x}_k \) represent the sequence of robot poses over time. At each time step, the latest pose \( \mathbf{x}_k \) is added to the factor graph along with new factors derived from LIO (odometry) and map-matching. Older poses such as \( \mathbf{x}_i \) for which \( t_i < t_k - \tau \) are marginalized out to limit the graph size.

The process of marginalization involves removing a variable (pose) from the graph while preserving its influence on the remaining variables. Suppose the state vector is partitioned as:
\begin{equation}
	\mathbf{x} = \begin{bmatrix} \mathbf{x}_m \\ \mathbf{x}_r \end{bmatrix},
\end{equation}
where \( \mathbf{x}_m \) are the variables to be marginalized (e.g., old poses outside the lag window), and \( \mathbf{x}_r \) are the variables retained in the active window. The joint probability distribution is expressed as:
\begin{equation}
	P(\mathbf{x}_m, \mathbf{x}_r) = P(\mathbf{x}_m | \mathbf{x}_r) P(\mathbf{x}_r).
\end{equation}
To obtain a reduced representation, we marginalize out \( \mathbf{x}_m \):
\begin{equation}
	P(\mathbf{x}_r) = \int P(\mathbf{x}_m, \mathbf{x}_r) \, d\mathbf{x}_m.
\end{equation}

GTSAM uses the Schur complement to perform this marginalization operation. As a result, a new dense factor is introduced between variables in \( \mathbf{x}_r \) that were connected to \( \mathbf{x}_m \), preserving the influence of the eliminated variables.

\begin{figure}[htbp]
	\centering
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/fixed_lag_smooth_a.png}
		 \captionsetup{width=2\textwidth}
		\caption{The initial factor graph contains active states \( \mathbf{x}_1 \) to \( \mathbf{x}_3 \), linked through LiDAR-inertial odometry (LIO) and map-matching factors. }
		\label{fig:bayes-a}
	\end{subfigure}
		\vskip\baselineskip
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/fixed_lag_smooth_b.png}
		\captionsetup{width=2\textwidth}
		\caption{A new state \( \mathbf{x}_4 \) is introduced, remaining within the lag window. }
		\label{fig:bayes-b}
	\end{subfigure}
	\vskip\baselineskip
	\begin{subfigure}[b]{0.7\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/fixed_lag_smooth_c.png}
		\captionsetup{width=1.2\textwidth}
		
		\caption{As state \( \mathbf{x}_5 \) is added, the oldest state \( \mathbf{x}_1 \) is marginalized and its influence is preserved via a prior factor. 
		}
		\label{fig:bayes-c}
	\end{subfigure}
    \vskip\baselineskip
	\begin{subfigure}[b]{0.7\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/fixed_lag_smooth_d.png}
		\captionsetup{width=1.2\textwidth}
		\caption{At time \( t_6 \), \( \mathbf{x}_6 \) is introduced and \( \mathbf{x}_2 \) is marginalized, maintaining a fixed-lag window over the most recent four states. 
	}
		\label{fig:bayes-d}
	\end{subfigure}
    \vskip\baselineskip
	\caption{Demonstration of fixed-lag smoothing with a lag size of \( n = 4 \) in a factor graph-based localization system.}
	\label{fig:fixed-lag-example}
\end{figure}

The process of fixed-lag smoothing is illustrated in Figure~\ref{fig:fixed-lag-example} using a temporal window of length \( n = 4 \). Initially (Figure~\ref{fig:bayes-a}), the system contains states \( \mathbf{x}_1 \), \( \mathbf{x}_2 \), and \( \mathbf{x}_3 \), connected by odometry and map-matching factors. As pose \( \mathbf{x}_4 \) is added at time \( t_4 \) (Figure~\ref{fig:bayes-b}), it links to \( \mathbf{x}_3 \) through a new odometry factor and includes a map constraint. Since all poses \( \mathbf{x}_1 \) to \( \mathbf{x}_4 \) lie within the lag window \( [t_1, t_4] \), the graph remains fully active and no marginalization occurs.


When the system reaches time step \( t_5 \), pose \( \mathbf{x}_5 \) is added, along with its corresponding odometry and map-matching constraints. At this point, pose \( \mathbf{x}_1 \) falls outside the fixed-lag window \( [t_2, t_5] \). As a result, it is marginalized. This process eliminates \( \mathbf{x}_1 \) from the graph but replaces its influence with a dense prior that connects to the remaining relevant states, particularly \( \mathbf{x}_2 \) and beyond (Figure~\ref{fig:bayes-c}). The marginalized node is no longer directly optimized, but its effect is preserved through conditional probability in the form of a Bayes net.

At the next step, time \( t_6 \), a new pose \( \mathbf{x}_6 \) is introduced. To maintain the lag window \( [t_3, t_6] \), the system marginalizes pose \( \mathbf{x}_2 \), introducing another dense factor that connects to the active variables. The current active states now consist of \( \mathbf{x}_3 \), \( \mathbf{x}_4 \), \( \mathbf{x}_5 \), and \( \mathbf{x}_6 \), all within the sliding lag window (Figure~\ref{fig:bayes-d}).

This incremental update mechanism, implemented using the GTSAM library, maintains a bounded graph size by marginalizing old variables into Bayes net conditionals while preserving historical information. Recent poses within the active window are kept in a factor graph and updated via iSAM2, striking a balance between computational efficiency and estimation accuracy for real-time localization.

\subsection{Outlier Rejection via Mahalanobis Distance}

To ensure robust fusion of odometry and scan–map correction constraints, we apply a probabilistic outlier test based on the Mahalanobis distance. Let the correction measurement be the 6-DOF pose  
\(\mathbf{z} = [x, y, z, \phi, \theta, \psi]^\mathsf{T}\)  
provided by the NDT-based scan matching, with associated covariance \(\mathbf{\Sigma}_z\). Given the current filter estimate \(\hat{\mathbf{z}}\), we compute the innovation  
\[
\Delta \mathbf{z} \;=\; \mathbf{z} - \hat{\mathbf{z}}.
\]  
The squared Mahalanobis distance is then
\begin{equation}
	\label{eq:mahalanobis}
	D_M^2 \;=\;\Delta \mathbf{z}^\mathsf{T}\,\mathbf{\Sigma}_z^{-1}\,\Delta \mathbf{z}.
\end{equation}
Under the Gaussian assumption, \(D_M^2\) follows a \(\chi^2\) distribution with \(d=6\) degrees of freedom. We reject the correction if
\begin{equation}
	\label{eq:threshold}
	D_M^2 \;>\;\chi^2_{d,\,0.95},
\end{equation}
where \(\chi^2_{d,\,0.95}\) is the 95\% quantile of the \(\chi^2_d\) distribution. Otherwise, the measurement is accepted and incorporated into the factor graph. This criterion effectively discards implausible scan-matching corrections while preserving valid global pose updates.


\section{Dynamic Object Detection and Removal}

\subsection{Dynamic Object Detection}

To identify and remove dynamic objects from the LiDAR point cloud, this work utilizes a deep learning-based 3D object detection pipeline based on CenterPoint\cite{yin2021center}. The model is developed and trained using the MMDetection3D framework\cite{mmdet3d2020}, and is deployed for real-time inference using the Autoware-compatible implementation\cite{autoware_universe}.

The detection architecture adopts the two-stage design of CenterPoint\cite{yin2021center}, as shown in Figure \ref{fig:Overview-CenterPoint }. First, the raw LiDAR point cloud is voxelized and processed by a voxel feature encoder and a PointPillars-based sparse convolutional backbone\cite{lang2019pointpillars}, which extracts high-level spatial features. These features are projected into a Bird’s-Eye View (BEV) map, enabling efficient object detection. A keypoint-based detection head then identifies object centers and regresses 3D bounding box attributes, including size, orientation, and velocity. The final output includes class-labeled 3D detections, which are used to support dynamic object removal.

Instead of training from scratch, this work utilizes a pretrained CenterPoint model trained on the nuScenes dataset \cite{caesar2020nuscenes}, comprising over 28k LiDAR frames. The model includes five object classes: CAR, TRUCK, BUS, BICYCLE, and PEDESTRIAN. These categories cover the primary dynamic agents present in urban driving scenarios. After training, the model is exported to the ONNX (Open Neural Network Exchange) format, which supports cross-platform deployment and enables hardware-accelerated inference using TensorRT.




\subsection{Detected Object removal }

Following object detection, dynamic elements are removed from the LiDAR point cloud using their predicted 3D bounding boxes. For each detected object, its position, dimensions, and orientation are used to define an oriented 3D cropping volume. These bounding boxes are expanded slightly with a fixed margin to ensure full exclusion of the object’s points, accounting for detection uncertainty or sensor noise.

Each bounding box is then used to define a geometric filter in 3D space. The removal process involves identifying all LiDAR points that fall within the transformed bounding box and excluding them from the original scan. This is performed iteratively for all detected dynamic objects in each frame. The result is a filtered point cloud that retains only static environmental features, which are then used for downstream localization tasks. 


\section{Environmental Noise Simulation}
\label{sec:env_noise_sim}


To quantify the resilience of our localization pipeline under a spectrum of adverse environmental conditions, we adapt the physically‐grounded framework of \cite{teufel2022realistic}, which realistically models rain, snow, and fog in LiDAR point clouds
 based on physical and empirical fundamentals.Rather than reproducing intensity effects, we concentrate solely on the geometric impact and generalize their approach into four spatial perturbations applied to each live scan immediately prior to registration



\begin{enumerate}
	\item \textbf{Removal of True Returns}
	In dense fog or heavy precipitation, many pulses are absorbed or scattered, never returning to the sensor. We mimic this by randomly deleting a fixed percentage of genuine points. Higher removal rates correspond to thicker fog or more intense rainfall conditions. 
	\item \textbf{Positional Jitter:} Droplet and flake impacts introduce small, random range and angle errors. We model this with zero-mean Gaussian noise  (standard deviation $\sigma_{\rm jitter}$ ) applied independently to each surviving point's $x,y,z$ coordinates.
	
	\item \textbf{Backscatter Point Modification:} A common artifact in foggy or drizzly conditions is a faint false return that appears closer to the sensor than the true object—produced when the beam scatters off a droplet mid-air. To simulate these near-field echoes, we select a fraction of points and “pull” them toward the LiDAR by resampling their range from an exponential distribution (so most of these modified points sit very close to the sensor).
	
	\item \textbf{Insert False Points:}Severe precipitation can yield completely spurious returns with no real obstacle. We inject these false points  by sampling a new, shorter range from an exponential law and appending an extra point at that location. 
	
\end{enumerate}

\subsubsection{Composite Noise Levels}
\label{sec:composite_noise}
\begin{table}[H]
	\centering
	\caption{Composite noise parameterizations and corresponding environmental severity levels}
	\label{tab:noise_levels}
	\begin{tabular}{l c c c c c c}
		\toprule
		\textbf{Level}   & \textbf{Removal Rate} & \(\sigma_{\rm jitter}\) & \textbf{Modify Rate} &  \textbf{Insert Rate} \\
		& (\,\%)                & (m)                     & (\,\%)              & (\,\%)          \\
		\midrule
		Mild      & 5   & 0.02 & 2   & 2.0 \\
		Moderate  & 15  & 0.05 & 3   & 2.0 \\
		Severe    & 25  & 0.10 & 5   & 5.0 \\
		\bottomrule
	\end{tabular}
\end{table}


We define three severity tiers—\textit{Mild}, \textit{Moderate}, and \textit{Severe}— by varying the Removal, Modify, and Insert rates.  Table \ref{tab:noise_levels} summarizes these settings and their analogues:



